{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source \n",
    " - https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/\n",
    " \n",
    "## Agent\n",
    "\n",
    "![](https://i2.wp.com/adventuresinmachinelearning.com/wp-content/uploads/2018/02/Reinforcement-learning-environment.png?w=381&ssl=1)\n",
    "\n",
    "## Actions\n",
    " - 0: forward one step\n",
    " - 1: backward to state 0\n",
    " \n",
    "## Rewards\n",
    " - 0: returns 10 only in state 4\n",
    " - 1: returns always 2\n",
    " \n",
    "\n",
    "## Environment\n",
    " \n",
    "![](https://adventuresinmachinelearning.com/wp-content/uploads/2018/02/NChain-illustration.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('NChain-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What `env.step(1)` returns\n",
    " \n",
    " - The new state after the action\n",
    " - The reward due to the action\n",
    " - Whether the game is “done” or not – the NChain game is done after 1,000 steps\n",
    " - Debugging information – not relevant in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2, False, {})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)# 1:backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2, False, {})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)# 1:backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2, False, {})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1) # 1:backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, False, {})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0) # 0:forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2, False, {})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0) # 0:forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, False, {})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0) # 0:forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 0, False, {})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0) # 0:forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2, False, {})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)# 1:backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2, False, {})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)# 1:backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive RL\n",
    "\n",
    "Reward table\n",
    " - $n_{states} \\times n_{actions}$ = $[5 \\times 2]$\n",
    " - Each entry $r_{s,a}$ the sum of the rewards that the agent has received in the past while taking action a in state s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_sum_reward_agent(env, num_episodes=500):\n",
    "    # this is the table that will hold our summated rewards for\n",
    "    # each action in each state\n",
    "    r_table = np.zeros((5, 2))\n",
    "    for g in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.sum(r_table[s, :]) == 0:\n",
    "                # make a random selection of actions\n",
    "                a = np.random.randint(0, 2)\n",
    "            else:\n",
    "                # select the action with highest cummulative reward\n",
    "                a = np.argmax(r_table[s, :])\n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            r_table[s, a] += r\n",
    "            s = new_s #  the state s is updated to new_s – the new state \n",
    "    return r_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examining the results above, you can observe that the most common state for the agent to be in is the first state, seeing as any action 1 will bring the agent back to this point. The least occupied state is state 4, as it is difficult for the agent to progress from state 0 to 4 without the action being “flipped” and the agent being sent back to state 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     0., 641190.],\n",
       "       [     0., 127442.],\n",
       "       [     0.,  25312.],\n",
       "       [     0.,   5106.],\n",
       "       [     0.,   3252.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_sum_reward_agent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearly – something is wrong with this table.\n",
    "Locked In\n",
    "> First, once there is a reward stored in one of the columns, the agent will always choose that action from that point on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delayed reward reinforcement learning\n",
    "\n",
    "Q-value\n",
    "\n",
    "$$\n",
    "Q(s,a) = Q(s,a) + \\alpha(r + \\gamma max_{a'} Q(s',a') - Q(s,a))\n",
    "$$\n",
    "\n",
    " - $\\alpha$ learning rate\n",
    " - $\\gamma$ discount factor\n",
    " - target: $r + \\gamma max_{a'} Q(s',a')$\n",
    " - prediction : $Q(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_with_table(env, num_episodes=500):\n",
    "    q_table = np.zeros((5, 2))\n",
    "    y = 0.95\n",
    "    lr = 0.8\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.sum(q_table[s,:]) == 0:\n",
    "                # make a random selection of actions\n",
    "                a = np.random.randint(0, 2)\n",
    "            else:\n",
    "                # select the action with largest q value in state s\n",
    "                a = np.argmax(q_table[s, :])\n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            q_table[s, a] += r + lr*(y*np.max(q_table[new_s, :]) - q_table[s, a]) # Q learnning rule\n",
    "            s = new_s\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , 27.57202638],\n",
       "       [28.18089044,  0.        ],\n",
       "       [ 0.        , 28.8523263 ],\n",
       "       [34.23301915,  0.        ],\n",
       "       [33.68482516,  0.        ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_learning_with_table(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon-greedy (Mutation)\n",
    "\n",
    "Locked in \n",
    "> initial bad decisions may continue\n",
    "\n",
    "Mutation, random actions, can escape from locked-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_q_learning_with_table(env, num_episodes=500):\n",
    "    q_table = np.zeros((5, 2))\n",
    "    y = 0.95\n",
    "    eps = 0.5\n",
    "    lr = 0.8\n",
    "    decay_factor = 0.999\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        eps *= decay_factor # mutation/innovation frequency decreases over time == annealing\n",
    "        done = False\n",
    "        while not done:\n",
    "            # make a random selection of actions\n",
    "            if np.random.random() < eps or np.sum(q_table[s, :]) == 0:\n",
    "                a = np.random.randint(0, 2)\n",
    "            # select the action with highest cummulative reward\n",
    "            else:\n",
    "                a = np.argmax(q_table[s, :])\n",
    "            # pdb.set_trace()\n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            q_table[s, a] += r + lr * (y * np.max(q_table[new_s, :]) - q_table[s, a])\n",
    "            s = new_s\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[31.30841065, 36.76243193],\n",
       "       [32.40476918, 33.17920827],\n",
       "       [38.39178364, 33.83378057],\n",
       "       [42.79325837, 34.44995201],\n",
       "       [38.89323493, 34.00175548]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps_greedy_q_learning_with_table(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Finally we have a table which favors action 0 in state 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(table, env):\n",
    "    s = env.reset()\n",
    "    tot_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        a = np.argmax(table[s, :])\n",
    "        s, r, done, _ = env.step(a)\n",
    "        tot_reward += r\n",
    "    return tot_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_methods(env, num_iterations=10):\n",
    "    winner = np.zeros((3,))\n",
    "    for g in range(num_iterations):\n",
    "        m0_table = naive_sum_reward_agent(env, 500)\n",
    "        m1_table = q_learning_with_table(env, 500)\n",
    "        m2_table = eps_greedy_q_learning_with_table(env, 500)\n",
    "        m0 = run_game(m0_table, env)\n",
    "        m1 = run_game(m1_table, env)\n",
    "        m2 = run_game(m2_table, env)\n",
    "        w = np.argmax(np.array([m0, m1, m2]))\n",
    "        winner[w] += 1\n",
    "        print(\"Game {} of {}\".format(g + 1, num_iterations))\n",
    "    return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1 of 10\n",
      "Game 2 of 10\n",
      "Game 3 of 10\n",
      "Game 4 of 10\n",
      "Game 5 of 10\n",
      "Game 6 of 10\n",
      "Game 7 of 10\n",
      "Game 8 of 10\n",
      "Game 9 of 10\n",
      "Game 10 of 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2., 0., 8.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_methods(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As can be observed, of the 100 experiments the  eps-greedy, Q learning algorithm (i.e. the third model that was presented) wins 65 of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep RL\n",
    "\n",
    "$$\n",
    "\\text{loss} = (\\underbrace{r + \\gamma \\max_{a’} Q'(s’, a’)}_{\\text{target}} – \\underbrace{Q(s, a)}_{\\text{prediction}})^2\n",
    "$$\n",
    "\n",
    "![](https://i1.wp.com/adventuresinmachinelearning.com/wp-content/uploads/2018/03/Reinforcement-learning-Keras.png?zoom=2&resize=340%2C335&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(InputLayer(batch_input_shape=(1, 5)))\n",
    "model.add(Dense(10, input_shape=(5,), activation='sigmoid'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 of 1000\n",
      "Episode 101 of 1000\n",
      "Episode 201 of 1000\n",
      "Episode 301 of 1000\n",
      "Episode 401 of 1000\n",
      "Episode 501 of 1000\n",
      "Episode 601 of 1000\n",
      "Episode 701 of 1000\n",
      "Episode 801 of 1000\n",
      "Episode 901 of 1000\n"
     ]
    }
   ],
   "source": [
    "# now execute the q learning\n",
    "y = 0.95\n",
    "eps = 0.5\n",
    "decay_factor = 0.999\n",
    "num_episodes = 1000\n",
    "r_avg_list = []\n",
    "for i in range(num_episodes):\n",
    "    s = env.reset()\n",
    "    eps *= decay_factor\n",
    "    if i % 100 == 0:\n",
    "        print(\"Episode {} of {}\".format(i + 1, num_episodes))\n",
    "    done = False\n",
    "    r_sum = 0\n",
    "    while not done:\n",
    "        # epsilon greedy\n",
    "        if np.random.random() < eps:\n",
    "            a = np.random.randint(0, 2)\n",
    "        else:\n",
    "            # a: best action in state s\n",
    "            a = np.argmax(model.predict(np.identity(5)[s:s + 1]))\n",
    "        # new state, after taking best action a from s\n",
    "        new_s, r, done, _ = env.step(a)\n",
    "        \n",
    "        # Target is the reward r plus the discounted maximum of the predicted Q values for the new state, new_s. \n",
    "        target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))\n",
    "        \n",
    "        # Previous Q(s,a)\n",
    "        target_vec = model.predict(np.identity(5)[s:s + 1])[0]\n",
    "        \n",
    "        # we want the Keras model to learn to predict for state s and action a New Q(s,a).\n",
    "        target_vec[a] = target\n",
    "        model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)\n",
    "        s = new_s\n",
    "        r_sum += r\n",
    "    r_avg_list.append(r_sum / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 3\n",
    "np.identity(5)[s:s + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([66.7478  , 62.803864], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.identity(5)[s:s + 1])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This output looks sensible – we can see that the Q values for each state will favor choosing action 0 (moving forward) to shoot for those big, repeated rewards in state 4. Intuitively, this seems like the best strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[63.28666 , 61.928818],\n",
       "       [66.7478  , 62.803867],\n",
       "       [71.22221 , 63.97839 ],\n",
       "       [77.43433 , 65.56561 ],\n",
       "       [85.506065, 67.42528 ]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.identity(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
