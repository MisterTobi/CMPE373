{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "You try to get rid of for loops\n",
    "$$\n",
    "z = w^tx + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-vectorized\n",
    "z = 0\n",
    "x = [1,1,1,1,1,1,1]\n",
    "w = [1,1,1,1,1,1,1]\n",
    "b = 1\n",
    "for i in range(len(x)):\n",
    "    z += w[i] * x[i]\n",
    "z+=b\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorized\n",
    "import numpy as np\n",
    "np.dot(x,w)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "x = np.random.rand(10000000)\n",
    "w = np.random.rand(10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "z = np.dot(x,w)+b\n",
    "end = time.time()\n",
    "print('vectorized version takes ', (end-start), ' seconds')\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "z = 0\n",
    "for i in range(len(x)):\n",
    "    z += w[i] * x[i]\n",
    "z+=b\n",
    "end = time.time()\n",
    "print('un-vectorized version takes ', (end-start), ' seconds')\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time.time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figures/vectorization.png](figures/vectorization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible problems with  vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.random.randn(3)\n",
    "print(a)\n",
    "print(\"\\nProblem in Shape :\", a.shape)\n",
    "print('\\tnot certain, \\n\\ttoo much felxibility')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use expilict non-ambigous shape for matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(3,1)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nShape :\", a.shape)\n",
    "print('\\tcertain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.T)\n",
    "print(\"\\nShape :\", a.T.shape)\n",
    "print('\\tcertain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Cost Function\n",
    "The loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set.\n",
    "\n",
    "$$\n",
    "L(a,y) = - (y*log(a) - (1-y)*log(1-a))\n",
    "$$\n",
    "\n",
    "Cost function\n",
    "$$J(w,b) = (1/m) * \\sum(L(a[i],y[i]))$$\n",
    "\n",
    "$z = w^tx + b$ and $a = \\sigma(z) \\in [0,1]$\n",
    " - $y=1 \\Rightarrow L(a,y) = -log(a)$\n",
    " - $y=0 \\Rightarrow L(a,y) = -log(1-a)$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(0.01,1,0.01)\n",
    "L1 = -np.log(a)\n",
    "L0 = -np.log(1-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(a,L1,)\n",
    "plt.plot(a,L0)\n",
    "plt.grid(True) \n",
    "plt.ylabel('Cost Function'); \n",
    "plt.xlabel('hypothesis outputs a')\n",
    "plt.legend((\"y=1\", \"y=0\"), shadow=True, fancybox=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets calculate the derivatives $\\frac{dL}{dw}$ and $\\frac{dL}{db}$\n",
    "\n",
    "\n",
    "\n",
    ">\"Derivative, what are you doing?\"\n",
    "> - \"I am trying to make a difference\" \n",
    " \n",
    "\n",
    "Output loss function\n",
    "$$\n",
    "\\frac{dL}{da} = \\frac{-y}{a} + \\frac{1-y}{1-a}\n",
    "$$\n",
    "One step back\n",
    "$$\n",
    "\\frac{dL}{dz} = \\frac{dL}{da} \\frac{da}{dz} = \\frac{dL}{da} \\sigma(z) (1-\\sigma(z)) = \\frac{dL}{da} a(1-a)\n",
    "$$\n",
    "if yo do the math\n",
    "$$\n",
    "\\frac{dL}{dz} = \\frac{dL}{da} a(1-a) = [\\frac{-y}{a} + \\frac{1-y}{1-a}]a(1-a) = -y(1-a) + (1-y)a = a -y\n",
    "$$\n",
    "\n",
    "One more step back\n",
    "$$\n",
    "\\frac{dL}{dw} = \\frac{dL}{da}\\frac{da}{dz}\\frac{dz}{dw} = (a-y)\\frac{dz}{dw} = (a-y)x\n",
    "$$\n",
    "\n",
    "> after you calculate $\\frac{dL}{dz} = (a-y)$ you just multiply it by x to get $\\frac{dL}{dw}$\n",
    "\n",
    "and also\n",
    "\n",
    "$$\n",
    "\\frac{dL}{db} = \\frac{dL}{da}\\frac{da}{dz}\\frac{dz}{db} = (a-y)\\frac{dz}{db} = (a-y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figures/logisticregression.png](figures/logisticregression.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data\n",
    "x1 = np.array([1,1])\n",
    "x2 = np.array([1,0])\n",
    "x3 = np.array([0,1])\n",
    "x4 = np.array([0,0])\n",
    "#Parameters\n",
    "b = 1\n",
    "w = np.array([[1],\n",
    "              [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data matrix\n",
    "X = np.stack((x1,x2,x3,x4)).T\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.dot(w.T, X)\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting\n",
    "Z += b\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output based on threshold: AND operation\n",
    "A = Z > 2\n",
    "A = A.astype(int)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uAnd(x, w = np.array([1,1]), b = 1):\n",
    "    \"\"\"outputs AND operation\"\"\"\n",
    "    Z = np.dot(w.T, x) + b\n",
    "    A = Z > 2\n",
    "    return A.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uAnd([1,1]))\n",
    "print(uAnd([1,0]))\n",
    "print(uAnd([0,1]))\n",
    "print(uAnd([1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uAnd(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: How to make 'or'?\n",
    "Use same w and b!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figures/LRimplementation.png](figures/LRimplementation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X:  (2, 8)\n",
      "Shape of y:  (1, 8)\n",
      "Shape of w:  (2, 1)\n"
     ]
    }
   ],
   "source": [
    "# n=2: number of features \n",
    "# m=4: number of observations\n",
    "\n",
    "# X: Data is a nxm matrix\n",
    "#    Each column c of X[c,:] is another observation\n",
    "#    Each row corresponds to a feature \n",
    "X = np.array([\n",
    "        [1, 1, 0, 0,1,1,1,1],\n",
    "        [1, 0, 1, 0,1,0,0,0]\n",
    "    ])\n",
    "#True labels for and operation\n",
    "y = np.array([[1,0,0,0,1,0,0,0]])\n",
    "\n",
    "# w is a column vector with shape (n,1)\n",
    "w = np.array([[1],\n",
    "              [1]])\n",
    "# Bias\n",
    "b = 1\n",
    "\n",
    "print(\"Shape of X: \", X.shape)\n",
    "print(\"Shape of y: \", y.shape)\n",
    "print(\"Shape of w: \", w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Z:  (1, 8)\n",
      "Shape of A:  (1, 8)\n",
      "Shape of dLdz:  (1, 8)\n",
      "Shape of dLdw:  (2, 1)\n",
      "Shape of dLdb:  ()\n",
      "2 8\n"
     ]
    }
   ],
   "source": [
    "n, m = X.shape# n: number of features and m: number of observations\n",
    "\n",
    "Z = np.dot(w.T,X) + b     #Forward Pass: Vectorization Then broadcasting, Z shape is (1, m)\n",
    "A = 1 / 1 + np.exp(-Z)    #Sigmoid     :Vectorization, A shape is (1, m)\n",
    "dLdz = A - y              #Compute derivative : dLdz shape is (1, m)\n",
    "\n",
    "# Parameters to be optimized: w, b\n",
    "dLdw = np.dot(X,dLdz.T)/m #Vectorization, dw shape is (Nx, 1) \n",
    "dLdb = dLdz.sum()/m         #Vectorization, dz shape is (1, 1)\n",
    "\n",
    "print(\"Shape of Z: \", Z.shape)\n",
    "print(\"Shape of A: \", A.shape)\n",
    "print(\"Shape of dLdz: \", dLdz.shape)\n",
    "print(\"Shape of dLdw: \", dLdw.shape)\n",
    "print(\"Shape of dLdb: \", dLdb.shape)\n",
    "print(n,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z:  [[3 2 2 1 3 2 2 2]]\n",
      "A:  [[ 1.04978707  1.13533528  1.13533528  1.36787944  1.04978707  1.13533528\n",
      "   1.13533528  1.13533528]]\n",
      "dLdz:  [[ 0.04978707  1.13533528  1.13533528  1.36787944  0.04978707  1.13533528\n",
      "   1.13533528  1.13533528]]\n",
      "dLdw:  [[ 0.58011441]\n",
      " [ 0.15436368]]\n",
      "dLdb:  0.893016249261\n"
     ]
    }
   ],
   "source": [
    "print(\"Z: \", Z)\n",
    "print(\"A: \", A)\n",
    "print(\"dLdz: \", dLdz)\n",
    "print(\"dLdw: \", dLdw)\n",
    "print(\"dLdb: \", dLdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class logisticRegression():\n",
    "    def __init__(self, X, y, alpha = 0.01):\n",
    "        self.alpha = alpha # learning rate\n",
    "        self.X, self.y = X, y\n",
    "        self.n, self.m = X.shape # n: number of features and m: number of observations\n",
    "        self.w = np.random.randn(n,1) # w is a column vector with shape (n,1)\n",
    "        self.b = 0.5\n",
    "        \n",
    "    def fit(self, n_iterations=1000):\n",
    "        Z = np.dot(self.w.T,self.X) + self.b     #Vectorized output Z with shape is (1, m)\n",
    "        A = 1 / 1 + np.exp(-Z)    #Vectorized Sigmoid for A with shape is (1, m)\n",
    "        dLdz = A - self.y              #Compute derivative : dLdz shape is (1, m)\n",
    "        \n",
    "        # Parameters to be optimized: w, b\n",
    "        dLdw = np.dot(self.X,dLdz.T)/self.m #Vectorization, dw shape is (Nx, 1) \n",
    "        dLdb = dLdz.sum()/self.m         #Vectorization, dz shape is (1, 1)\n",
    "        \n",
    "        for i in range(n_iterations):\n",
    "            self.w = self.w - self.alpha * dLdw\n",
    "            self.b = self.b - self.alpha * dLdb\n",
    "    \n",
    "    def predict(self, newX):\n",
    "        Z = np.dot(self.w.T,newX) + self.b \n",
    "        A = 1 / 1 + np.exp(-Z)\n",
    "        return np.round(A)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.36521848e+13,   8.36521848e+13,   3.47697400e+06,\n",
       "          3.47697400e+06]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = logisticRegression(X,y)\n",
    "lr.fit()\n",
    "lr.predict(np.array([[1,1,0,0],[1,1,0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-10.94795233],\n",
       "        [ -6.04806377]]), -15.061672570008051)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.w, lr.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 796.,  484.,   17.,   11.,  796.,  484.,  484.,  484.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
